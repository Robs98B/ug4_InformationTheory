\documentclass[10pt,a4paper,oneside,onecolumn]{article}

\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{appendix}
\usepackage{minted}
\usepackage{titlesec}
\usepackage{xspace}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{lastpage}
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhead{}
\renewcommand{\headrulewidth}{0pt}
\cfoot{\thepage/\pageref{LastPage}}
\renewcommand*{\thepart}{\arabic{part}}
\titleformat{\part}[hang]
{\normalfont\Large\bf}{\partname\ \thepart:}{0.5em}{}[]
\titleformat{\section}[hang]
{\normalfont\normalsize\bf}{\thesection.}{0.5em}{}[]
\setlength{\parindent}{0pt}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newcommand*{\thesisTXT}{{\tt thesis.txt}\xspace}
\newcommand*{\receivedTXT}{{\tt received.txt}\xspace}
\newcommand*{\packetsTXT}{{\tt packets.txt}\xspace}
\newcommand*{\XOR}{{\tt XOR}\xspace}
\newcommand*{\eg}{e.g.\@}
\newcommand*{\ie}{i.e.\@}
\newcommand*{\iid}{i.i.d.\@}
\newcommand*{\norm}[1]{\ensuremath{\left|\left|#1\right|\right|}}
\newcommand*{\lbr}{\ensuremath{\left\{}}
\newcommand*{\rbr}{\ensuremath{\right\}}}
\newcommand*{\textapprox}{\raisebox{0.5ex}{\texttildelow}}

\title{Information Theory -- Assessment 1}
\author{Clemens Wolff (s0942284)}
\date{\vspace{-2em}}

\begin{document}
\maketitle\thispagestyle{fancy}


\part{Source coding}


\section{Character statistics}\label{sec:ex1}

The Python program proposed in Appendix~\ref{app:ex1} computes a probability
distribution $p$ over characters $x_n$ from a given input file by counting the
number of occurrences $c_{x_n}$ of each character $x_n$ in the file and
normalizing those counts by the total number of characters:
\begin{equation}
    p(x_n) = {c_{x_n} \over\sum\limits_{x_n} c_{x_n}}.
\end{equation}
Using this distribution\footnotemark, the program then computes the entropy:
\footnotetext{Sanity check: ``e'', ``t'', ``a'' and ``o'' are indeed the most
common letters in English \cite{oed}.}
\begin{equation}
    H(X_n) = -\sum\limits_{x_n} p(x_n) \cdot \log p(x_n).
\end{equation}
For \thesisTXT, we have $H(X_n) = 4.168$ bits.\footnotemark
\footnotetext{Sanity check: this is pretty close to the ``4.11'' in
\cite[p.~7]{it4}.}

\begin{table}[ht]
\centering
\begin{tabular}{| c c | c c |}
\hline
$x_n$ & $p(x_n)$ & $x_n$ & $p(x_n)$ \\
\hline
 ' '  & 0.166558 &   n   & 0.056932 \\
  a   & 0.068588 &   o   & 0.059740 \\
  b   & 0.014909 &   p   & 0.026120 \\
  c   & 0.025466 &   q   & 0.002442 \\
  d   & 0.027327 &   r   & 0.053147 \\
  e   & 0.094014 &   s   & 0.058728 \\
  f   & 0.016226 &   t   & 0.076727 \\
  g   & 0.017804 &   u   & 0.020545 \\
  h   & 0.030597 &   v   & 0.009116 \\
  i   & 0.071036 &   w   & 0.010950 \\
  j   & 0.001808 &   x   & 0.008732 \\
  k   & 0.005770 &   y   & 0.010464 \\
  l   & 0.036346 &   z   & 0.002247 \\
  m   & 0.027664 &       &          \\
\hline
\end{tabular}
\caption{Unigram probabilities for \thesisTXT}
\label{tbl:unigram-probs}
\end{table}


\section{Bigram statistics}\label{sec:ex2}

If we split a text into its constituent bigrams as a preprocessing step, we can
use the program of Section~\ref{sec:ex1} to compute the joint entropy $H(X_n,
X_{n+1})$ of character bigrams in a given file. For \thesisTXT, we
find: $H(X_n, X_{n+1}) = 7.572 \approx 1.82H(X_n)$ bits.\footnotemark
\footnotetext{Sanity check: this is pretty close to the ``7.6'' in
\cite[p.~10]{it4}.}

As expected, this number is lower than $2H(X_n)$. By $p(x,y) \le p(x) \cdot
p(y)$ and monotonicity of the logarithm we have $\forall X,Y: H(X, Y) \le H(X) +
H(Y)$ with equality only occurring if $X$ and $Y$ are independent
\cite[p.~138]{mackay}. Here, we $X$ and $Y$ are occurrences of characters in
English text which are not independent \cite[p.~22-24]{mackay}.

Rearranging the chain rule of entropy \cite[p.~139]{mackay} reveals that we can
also use the program of Section~\ref{sec:ex1} to compute the conditional
entropy:
\begin{equation}
    H(X_{n+1} | X_n) = H(X_n, X_{n+1}) - H(X_n).
\end{equation}
For \thesisTXT, we have: $H(X_{n+1} | X_n) = 3.404$ bits.


\section{Compression with known distributions}\label{sec:ex3}

We know \cite[p.~21]{it4} the number of bits $b$ that an arithmetic coder
will need to encode any sequence $s$ that has probability $p(s)$:
\begin{equation}
    b < -\log p(s) + 2.
\end{equation}

If we assume that all characters $x_0, x_1, \cdots, x_N$ in a file $f$ are
generated independent and identically distributed (\iid) from $p(x_n)$ we can
compute the probability of the entire file as:
\begin{equation}
    p(f) = \prod\limits_{x_n \in f} p(x_n).
\end{equation}
Taking the logarithm on both sides gives:
\begin{equation}\label{eq:unigram-logprob}
    \log p(f) = \sum\limits_{x_n \in f} \log p(x_n).
\end{equation}
We can now compute the maximum number of bits $b_1$ that we will need to encode
$f$ with an arithmetic coder using a unigram model:
\begin{equation}\label{eq:unigram-nbits}
    b_1 < -\log p(f) + 2 = 2 - \sum\limits_{x_n \in f} \log p(x_n).
\end{equation}
Performing this computation for \thesisTXT gives us: $b_1 < 1,433,834$ bits.

If we instead assume that only $x_0$, the first character in $f$, is drawn all
from $p(x_n)$ and all subsequent characters $x_1, x_2, \cdots, x_N$ are drawn
from $p(x_{n+1} | x_n)$, we can write the probability of the file as:
\begin{equation}
    p(f) = p(x_0) \cdot \prod\limits_{n = 0}^{N - 1} p(x_{n + 1} | x_n).
\end{equation}
Taking the logarithm on both sides gives:
\begin{equation}
    \log p(f) =
    \log p(x_0) + \sum\limits_{n = 0}^{N - 1} \log p(x_{n + 1} | x_n).
\end{equation}
Using Bayes Rule we can rewrite this as:
\begin{equation}
    \log p(f)
    = \log p(x_0)
    + \sum\limits_{n = 0}^{N - 1} \log {p(x_n, x_{n + 1}) \over p(x_n)}.
\end{equation}
We can simplify this further by applying properties of the logarithm:
\begin{equation}\label{eq:bigram-logprob}
    \log p(f)
    = \log p(x_0)
    + \sum\limits_{n = 0}^{N - 1} \log p(x_n, x_{n + 1}) - \log p(x_n).
\end{equation}
We can now compute the maximum number of bits $b_2$ that an arithmetic coder
will need to encode $f$ using a bigram model:
\begin{equation}\label{eq:bigram-nbits}
    b_2 < -\log p(f) + 2
    = 2 - \log p(x_0)
    - \sum\limits_{n = 0}^{N - 1} \log p(x_n, x_{n + 1}) + \log p(x_n).
\end{equation}
Performing this computation for \thesisTXT gives us: $b_2 < 1,171,193$ bits. We
use 18\% fewer bits if we base our arithmetic coder on a bigram model instead of
a unigram model!


\section{Compression with limited precision header}\label{sec:ex4}

Let us now consider the ``limited precision header'' coding scheme presented in
\cite{it-assign}: as previously, we will encode the file $f$ using a probability
distribution $p$ over character unigrams $i$ in the file, but we now also
include a description of this distribution as a header in the file. In order to
simplify things, we round the probabilities $p(i)$ to the next largest power of
two ${\hat q}(i)$ that we can represent with 8 bits:
\begin{equation}
    {\hat q}(i) = {1 \over 2^8} \cdot \ceil{2^8 \cdot p(i)}.
\end{equation}
Normalizing ${\hat q}(i)$ to make the probabilities sum to 1 gives us the
distribution $q(i)$ used for encoding:
\begin{equation}
    q(i) = {1 \over \sum\limits_{j} {\hat q}(j)} \cdot {\hat q}(i)
\end{equation}
We now have all the information necessary to apply equations
\eqref{eq:unigram-logprob} and \eqref{eq:unigram-nbits} in order to compute the
maximum number of bits $b_{q^1}$ necessary to encode $f$ using an arithmetic
coder.  For \thesisTXT we get $b_{q^1} < 1,435,079$ bits.

If we now assume that the alphabet $\mathcal{A} = \lbr a, b, \cdots, z,
\textvisiblespace \rbr$ used in the file is known by everyone (\ie all parties
agree that if they see a header containing probabilities $p_0, p_1, \cdots,
p_{\norm{\mathcal{A}}}$ they know which symbols to associate with the sequence
of probabilities), we can represent the header in exactly $(\norm{\mathcal{A}} -
1) \times 8 = 26 \times 8 = 208$ bits.\footnotemark

We thus need $b_{q^1} + 208 < 1,435,287$ bits in total to encode \thesisTXT.

\footnotetext{We have 27 elements in $\mathcal{A}$ but since since probabilities
sum to one we can omit the last element: it is simply one minus the sum of the
other elements.}

Note that we can relax the assumption that the alphabet is known to everyone by
increasing the header length by $\norm{\mathcal{A}} \times N + 8$ bits. The
header then contains each element in the alphabet encoded in some way using $N$
bits per symbol (\eg using ASCII) followed by the probability of that element.
We also define some sequence of 8 bits as the ``end of header'' symbol. Using
{\tt 00000000} for this symbol would be a good choice since the knowledge that a
character in the encoded message has probability zero is useless. We would
therefore never use this number to communicate the probability of an actual
member of our probability distribution, making {\tt 00000000} an unambiguous
sign for the ``end of header'' symbol.

If we want to use a limited precision header to encode a distribution over
character bigrams, the header is much larger than in the unigram case: if we
have an alphabet of $\norm{\mathcal{A}}$ characters, we have
$\norm{\mathcal{A}}^2$ character bigrams (\ie we need to send
$(\norm{\mathcal{A}} - 1)^2$ probabilities in the header -- a length of $5,824$
bits in the case of \thesisTXT). Applying equations \eqref{eq:bigram-logprob}
and \eqref{eq:bigram-nbits} to compute the maximum number of bits $b_{q^2}$
necessary to encode \thesisTXT using $q$ shows that the penalty for using this
incorrect distribution is also higher in the bigram case: we need $b_{q^2} <
1,463,686$ bits to encode the contents of \thesisTXT\@.

We thus need $b_{q^2} + 5,824 < 1,469,510$ bits in total to encode \thesisTXT.

\section{Compression with adaptation}\label{sec:ex5}

One way to avoid having to send a header describing the probability distribution
used to encode a file is to infer the distribution as we go along decoding. We
can do this, for example, using the Laplace prediction rule
\cite[p.~2]{it-assign}:
\begin{equation}
    p(x_{n+1}=a_i|x_{\le n}) = {k_i + 1 \over n + \norm{\mathcal{A}}},
\end{equation}
where $k_i$ is the number of times character $a_i$ occurs in the sub-string
$x_0, x_1, \cdots, x_n$.
The log probability of a file $f$ with characters $x_0, x_1, \cdots, x_N$ then
is:
\begin{equation}\label{eq:adaptive-unigram}
    \log p(f) =
    \sum\limits_{n = 0}^N \log {k_{x_n} + 1 \over n + \norm{\mathcal{A}}}.
\end{equation}
Appendix~\ref{app:ex5} implements a Python program that uses equations
\eqref{eq:adaptive-unigram} and \eqref{eq:unigram-nbits} to compute the maximum
number of bits $b_{a^1}$ required to encode a file with this scheme. For
\thesisTXT we find that $b_{a^1} < 1,434,024$. Note that we save roughly 1,000
bits over the unigram model in Section~\ref{sec:ex4} which is more than just the
size of the header.

Modifying this approach to make use of probabilities over character bigrams
instead of just unigrams is trivial: just replace equation
\eqref{eq:adaptive-unigram} with a bigram prediction model such as
\cite[p.~3]{it-assign}:
\begin{equation}\label{eq:adaptive-bigram}
    p(x_{n+1}=a_i|x_{n} = a_j, x_{<n})
    = {k_{i|j} + 1 \over n_j + \norm{\mathcal{A}}},
\end{equation}
where $k_{i|j}$ is the number of times we have seen the bigram $a_ja_i$ in
$x_0, x_1, \cdots, x_n$ and $n_j$ is the number of times we have conditioned a
probability on $a_j$ so far.
We can run the Python program of Appendix~\ref{app:ex5} using equations
\eqref{eq:adaptive-bigram} and \eqref{eq:unigram-nbits} to compute the maximum
number of bits $b_{a^2}$ required to encode a file with this scheme. For
\thesisTXT we find that $b_{a^2} < 1,175,380$. This is only \textapprox 4,000
bits (or less than 1\%) more than the number we found in Section~\ref{sec:ex3}
where the sender and receiver both knew the exact probability distribution used
to encode the message!


\part{Noisy channel coding}


\section{\XOR--ing packets}\label{sec:ex6}

Performing the \XOR operation between the string ``nutritious snacks'' and the
bytes with numerical values ``59 6 17 0 83 84 26 90 64 70 25 66 86 82 90 95 75''
results in the string ``User: s5559183948''


\section{Decoding packets from a digital fountain}\label{sec:ex7}

A ``digital fountain'' is a code where parts of the source message are \XOR--ed
together before being sent. Appendix~\ref{app:ex7} proposes a Python program
that acts as a decoder for this scheme. On a high level, the program works as
follows:

\begin{enumerate}
\item We are given as inputs a list of received packets and a list of source
      packets that were used to encode each of the received packets.
\item Find a packet that was encoded using only one source packet $s$, say,
      packet $n$.  If no such packet exists, the message can't be decoded.
\item We now know that the $n$\textsuperscript{th} character of the original
      message has to be the character with byte representation $v$.
\item Find all the packets that used packet $n$ as part of their encoding.
      \XOR these packets with $v$ and delete $n$ from their encoding list.
\item Repeat steps 2--4 until all the encoding lists are empty or until we find
      that the message can't be decoded.
\end{enumerate}

Using the program to decode \receivedTXT and \packetsTXT produces the string
``Password: X!3baA1z''. Packets 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
16, 17, 18 and 19 are used in the final message(leaving packets 1, 20, 21, 22
and 23 unused).


\section{Creating a code}\label{sec:ex8}


\begin{thebibliography}{9}

\bibitem{mackay}
    D.J.C. MacKay,
    \emph{Information Theory, Inference \& Learning Algorithms},
    Cambridge University Press,
    2002.

\bibitem{it4}
    I. Murray,
    \emph{Practical compression 2: stream codes},
    Lecture at the University of Edinburgh,
    15-22 Oct 2013,
    available online at \url{http://goo.gl/Q2JkXm}

\bibitem{it-assign}
    I. Murray,
    \emph{Information Theory — Assessed Assignment},
    University of Edinburgh,
    21 Oct 2013,
    available online at \url{http://goo.gl/7QjRMc}

\bibitem{oed}
    Oxford Dictionary,
    \emph{What is the frequency of the letters in the alphabet in English?},
    Oxford University Press,
    retrieved 22 Oct 2013 from \url{http://goo.gl/DIl3uQ}

\end{thebibliography}


\onecolumn
\appendixpage
\appendix

\section{Code listing}

\subsection{ex1.py}\label{app:ex1}
\inputminted{python}{../src/ex1.py}
\newpage

\subsection{ex2.py}\label{app:ex2}
\inputminted{python}{../src/ex2.py}
\newpage

\subsection{ex3.py}\label{app:ex3}
\inputminted{python}{../src/ex3.py}
\newpage

\subsection{ex4.py}\label{app:ex4}
\inputminted{python}{../src/ex4.py}
\newpage

\subsection{ex5.py}\label{app:ex5}
\inputminted{python}{../src/ex5.py}
\newpage

\subsection{ex6.py}\label{app:ex6}
\inputminted{python}{../src/ex6.py}
\newpage

\subsection{ex7.py}\label{app:ex7}
\inputminted{python}{../src/ex7.py}
\newpage

\subsection{util.py}\label{app:util}
\inputminted{python}{../src/util.py}

\end{document}

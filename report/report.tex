\documentclass[10pt,a4paper,oneside,onecolumn]{article}

\usepackage{hyperref}
\usepackage{appendix}
\usepackage{minted}
\usepackage{titlesec}
\usepackage{xspace}

\renewcommand*{\thepart}{\arabic{part}}
\titleformat{\part}[hang]
{\normalfont\Large\bf}{\partname\ \thepart:}{0.5em}{}[]
\titleformat{\section}[hang]
{\normalfont\normalsize\bf}{\thesection.}{0.5em}{}[]
\setlength{\parindent}{0pt}

\newcommand*{\thesisTXT}{{\tt thesis.txt}\xspace}
\newcommand*{\receivedTXT}{{\tt received.txt}\xspace}
\newcommand*{\packetsTXT}{{\tt packets.txt}\xspace}
\newcommand*{\XOR}{{\tt XOR}\xspace}
\newcommand*{\eg}{e.g.\xspace}

\title{Information Theory - Assessment 1}
\author{Clemens Wolff (s0942284)}
\date{\vspace{-2em}}

\begin{document}
\maketitle


\part{Source coding}

\section{Character statistics}\label{sec:ex1}

The Python program proposed in Appendix~\ref{app:ex1} computes a probability
distribution over characters from a given input file by counting the number of
occurrences $c_{x_n}$ of each character $x_n$ in the file and normalizing those
counts by the total number of characters to obtain:

\begin{center}
    $p(x_n) = {c_{x_n} \over\sum\limits_{x_n} c_{x_n}}$
\end{center}

\begin{table}[ht]
\centering
\begin{tabular}{| c c | c c |}
\hline
$x_n$ & $p(x_n)$ & $x_n$ & $p(x_n)$ \\
\hline
 ' '  & 0.166558 &   n   & 0.056932 \\
  a   & 0.068588 &   o   & 0.059740 \\
  b   & 0.014909 &   p   & 0.026120 \\
  c   & 0.025466 &   q   & 0.002442 \\
  d   & 0.027327 &   r   & 0.053147 \\
  e   & 0.094014 &   s   & 0.058728 \\
  f   & 0.016226 &   t   & 0.076727 \\
  g   & 0.017804 &   u   & 0.020545 \\
  h   & 0.030597 &   v   & 0.009116 \\
  i   & 0.071036 &   w   & 0.010950 \\
  j   & 0.001808 &   x   & 0.008732 \\
  k   & 0.005770 &   y   & 0.010464 \\
  l   & 0.036346 &   z   & 0.002247 \\
  m   & 0.027664 &       &          \\
\hline
\end{tabular}
\caption{Unigram probabilities for \thesisTXT}
\label{tbl:unigram-probs}
\end{table}

Using this distribution\footnotemark, the program then computes the entropy:
\footnotetext{Sanity check: ``e'', ``t'', ``a'' and ``o'' are indeed the most
common letters in English \cite{oed}.}

\begin{center}
    $H(X_n) = -\sum\limits_{x_n} p(x_n) \cdot \log p(x_n)$
\end{center}

For \thesisTXT, we have:

\begin{center}
    $H(X_n) = 4.168$\footnotemark
\end{center}
\footnotetext{Sanity check: this is pretty close to the ``4.11'' in
\cite[p.~7]{it4}.}

\section{Bigram statistics}\label{sec:ex2}

If we split a text into its constituent bigrams as a pre-procesing step, we can
use the program of Section~\ref{sec:ex1} to compute the joint entropy $H(X_n,
X_{n+1})$ of character bigrams in a given file. For \thesisTXT, we find:

\begin{center}
    $H(X_n, X_{n+1}) = 7.572 \approx 1.82H(X_n)$\footnotemark
\end{center}
\footnotetext{Sanity check: this is pretty close to the ``7.6'' in
\cite[p.~10]{it4}.}

As expected, this number is lower than $2H(X_n)$. By $p(x,y) \le p(x) \cdot
p(y)$ and monotonicity of the logarithm we have $\forall X,Y: H(X, Y) \le H(X) +
H(Y)$ with equality only occuring if $X$ and $Y$ are independent
\cite[p.~138]{mackay}. Here, we $X$ and $Y$ are occurrences of characters in
English text which are not independent \cite[p.~22-24]{mackay}.

Rearanging the chain rule of entropy \cite[p.~139]{mackay} reveals that we can
also use the program of Section~\ref{sec:ex1} to compute the conditional
entropy:

\begin{center}
    $H(X_{n+1} | X_n) = H(X_n, X_{n+1}) - H(X_n)$.
\end{center}

For \thesisTXT, we have:

\begin{center}
    $H(X_{n+1} | X_n) = 3.404$.
\end{center}


\section{Compression with known distributions}\label{sec:ex3}
\section{Compression with limited precision header}\label{sec:ex4}
\section{Compression with adaptation}\label{sec:ex5}


\part{Noisy channel coding}

\section{\XOR-ing packets}\label{sec:ex6}

Performing the \XOR operation between the string ``nutritious snacks'' and the
bytes with numerical values ``59 6 17 0 83 84 26 90 64 70 25 66 86 82 90 95 75''
results in the string ``User: s5559183948''

\section{Decoding packets from a digital fountain}\label{sec:ex7}

A ``digital fountain'' is a code where parts of the source message are \XOR-ed
together before being sent. Appendix~\ref{app:ex7} proposes a Python program to
decode this code. On a high level, the program works as follows:

\begin{enumerate}
\item We are given as inputs a list of received packets and a list of source
      packets that were used to encode each of the received packets.
\item Find a packet that was encoded using only one source packet $s$, say,
      packet $n$.  If no such packet exists, the message can't be decoded.
\item We now know that the $n$\textsuperscript{th} character of the original
      message has to be the character with byte representation $v$.
\item Find all the packets that used packet $n$ as part of their encoding.
      \XOR these packets with $v$ and delete $n$ from their encoding list.
\item Repeat steps 2-4 until all the encoding lists are empty or until we find
      that the message can't be decoded.
\end{enumerate}

Using the program to decode the received bytes in \receivedTXT under the
assumption of the transformations in \packetsTXT produces the string ``Password:
X!3baA1z''. During the execution of the program, packets 2, 3, 4, 5, 6, 7, 8, 9,
10, 11, 12, 13, 14, 15, 16, 17, 18 and 19 are used (leaving packets 1, 20, 21,
22 and 23 unused).

\section{Creating a code}\label{sec:ex8}


\begin{thebibliography}{9}

\bibitem{mackay}
    D.J.C. MacKay,
    \emph{Information Theory, Inference \& Learning Algorithms},
    Cambridge University Press,
    2002.

\bibitem{it4}
    I. Murray,
    \emph{Practical compression 2: stream codes},
    Lecture at the University of Edinburgh,
    15-22 Oct 2013,
    available on-line at \url{http://goo.gl/Q2JkXm}

\bibitem{oed}
    Oxford Dictionary,
    \emph{What is the frequency of the letters in the alphabet in English?},
    Oxford University Press,
    retrieved 22 Oct 2013 from \url{http://goo.gl/DIl3uQ}

\end{thebibliography}


\onecolumn
\appendixpage
\appendix

\section{Code listing}

\subsection{ex1.py}\label{app:ex1}
\inputminted{python}{../src/ex1.py}
\newpage

\subsection{ex2.py}\label{app:ex2}
\inputminted{python}{../src/ex2.py}
\newpage

\subsection{ex6.py}\label{app:ex6}
\inputminted{python}{../src/ex6.py}
\newpage

\subsection{ex7.py}\label{app:ex7}
\inputminted{python}{../src/ex7.py}
\newpage

\subsection{util.py}\label{app:util}
\inputminted{python}{../src/util.py}

\end{document}

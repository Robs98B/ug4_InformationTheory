\documentclass[10pt,a4paper,twoside,twocolumn]{article}

\usepackage{appendix}
\usepackage{minted}
\usepackage{titlesec}
\usepackage{xspace}
\usepackage[margin=0.9in]{geometry}

\renewcommand*{\thepart}{\arabic{part}}
\titleformat{\part}[hang]
{\normalfont\Large\bf}{\partname\ \thepart:}{0.5em}{}[]
\titleformat{\section}[hang]
{\normalfont\normalsize\bf}{\thesection.}{0.5em}{}[]

\setlength{\parindent}{0pt}

\newcommand*{\thesisTXT}{{\tt thesis.txt}\xspace}

\title{Information Theory - Assessment 1}
\author{Clemens Wolff (s0942284)}
\date{\vspace{-2em}}

\begin{document}
\maketitle


\part{Source coding}

\section{Character statistics}
The Python program proposed in Appendix~\ref{app:ex1} computes a probability
distribution over characters from a given input file by counting the number of
occurrences $c_{x_n}$ of each character $x_n$ in the file and normalizing those
counts by the total number of characters $C = \sum\limits_{x_n} c_{x_n}$ to
obtain $p(x_n) = {1 \over C} \cdot c_{x_n}$.

It then computes the entropy $H(X_n)$ of the distribution using the formula:
\begin{center}
    $H(X_n) = -\sum\limits_{x_n} p(x_n) \cdot log~p(x_n)$
\end{center}
Analysing the file \thesisTXT with this program we find $H(X_n) = 4.168$.

\section{Bigram statistics}
\section{Compression with known distributions}
\section{Compression with limited precision header}
\section{Compression with adaptation}


\part{Noisy channel coding}

\section{XOR-ing packets}
\section{Decoding packets from a digital fountain}
\section{Creating a code}


\onecolumn
\appendixpage
\appendix

\section{Code listing}

\subsection{ex1.py}\label{app:ex1}
\inputminted{python}{../src/ex1.py}

\subsection{ex2.py}\label{app:ex2}
\inputminted{python}{../src/ex2.py}

\subsection{ex6.py}\label{app:ex6}
\inputminted{python}{../src/ex6.py}

\subsection{ex7.py}\label{app:ex7}
\inputminted{python}{../src/ex7.py}

\subsection{util.py}\label{app:util}
\inputminted{python}{../src/util.py}

\end{document}

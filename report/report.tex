\documentclass[10pt,a4paper,oneside,onecolumn]{article}

\usepackage{hyperref}
\usepackage{appendix}
\usepackage{minted}
\usepackage{titlesec}
\usepackage{xspace}
\usepackage{mathtools}

\renewcommand*{\thepart}{\arabic{part}}
\titleformat{\part}[hang]
{\normalfont\Large\bf}{\partname\ \thepart:}{0.5em}{}[]
\titleformat{\section}[hang]
{\normalfont\normalsize\bf}{\thesection.}{0.5em}{}[]
\setlength{\parindent}{0pt}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newcommand*{\thesisTXT}{{\tt thesis.txt}\xspace}
\newcommand*{\receivedTXT}{{\tt received.txt}\xspace}
\newcommand*{\packetsTXT}{{\tt packets.txt}\xspace}
\newcommand*{\XOR}{{\tt XOR}\xspace}
\newcommand*{\eg}{e.g.\xspace}
\newcommand*{\iid}{i.i.d.\xspace}
\newcommand*{\norm}[1]{\ensuremath{\left|\left|#1\right|\right|}}
\newcommand*{\lbr}{\ensuremath{\left\{}}
\newcommand*{\rbr}{\ensuremath{\right\}}}

\title{Information Theory - Assessment 1}
\author{Clemens Wolff (s0942284)}
\date{\vspace{-2em}}

\begin{document}
\maketitle


\part{Source coding}

\section{Character statistics}\label{sec:ex1}

The Python program proposed in Appendix~\ref{app:ex1} computes a probability
distribution over characters from a given input file by counting the number of
occurrences $c_{x_n}$ of each character $x_n$ in the file and normalizing those
counts by the total number of characters to obtain:

\begin{center}
    $p(x_n) = {c_{x_n} \over\sum\limits_{x_n} c_{x_n}}$
\end{center}

\begin{table}[ht]
\centering
\begin{tabular}{| c c | c c |}
\hline
$x_n$ & $p(x_n)$ & $x_n$ & $p(x_n)$ \\
\hline
 ' '  & 0.166558 &   n   & 0.056932 \\
  a   & 0.068588 &   o   & 0.059740 \\
  b   & 0.014909 &   p   & 0.026120 \\
  c   & 0.025466 &   q   & 0.002442 \\
  d   & 0.027327 &   r   & 0.053147 \\
  e   & 0.094014 &   s   & 0.058728 \\
  f   & 0.016226 &   t   & 0.076727 \\
  g   & 0.017804 &   u   & 0.020545 \\
  h   & 0.030597 &   v   & 0.009116 \\
  i   & 0.071036 &   w   & 0.010950 \\
  j   & 0.001808 &   x   & 0.008732 \\
  k   & 0.005770 &   y   & 0.010464 \\
  l   & 0.036346 &   z   & 0.002247 \\
  m   & 0.027664 &       &          \\
\hline
\end{tabular}
\caption{Unigram probabilities for \thesisTXT}
\label{tbl:unigram-probs}
\end{table}

Using this distribution\footnotemark, the program then computes the entropy:
\footnotetext{Sanity check: ``e'', ``t'', ``a'' and ``o'' are indeed the most
common letters in English \cite{oed}.}

\begin{center}
    $H(X_n) = -\sum\limits_{x_n} p(x_n) \cdot \log p(x_n)$
\end{center}

For \thesisTXT, we have:

\begin{center}
    $H(X_n) = 4.168$\footnotemark
\end{center}
\footnotetext{Sanity check: this is pretty close to the ``4.11'' in
\cite[p.~7]{it4}.}

\section{Bigram statistics}\label{sec:ex2}

If we split a text into its constituent bigrams as a pre-procesing step, we can
use the program of Section~\ref{sec:ex1} to compute the joint entropy $H(X_n,
X_{n+1})$ of character bigrams in a given file. For \thesisTXT, we find:

\begin{center}
    $H(X_n, X_{n+1}) = 7.572 \approx 1.82H(X_n)$\footnotemark
\end{center}
\footnotetext{Sanity check: this is pretty close to the ``7.6'' in
\cite[p.~10]{it4}.}

As expected, this number is lower than $2H(X_n)$. By $p(x,y) \le p(x) \cdot
p(y)$ and monotonicity of the logarithm we have $\forall X,Y: H(X, Y) \le H(X) +
H(Y)$ with equality only occuring if $X$ and $Y$ are independent
\cite[p.~138]{mackay}. Here, we $X$ and $Y$ are occurrences of characters in
English text which are not independent \cite[p.~22-24]{mackay}.

Rearanging the chain rule of entropy \cite[p.~139]{mackay} reveals that we can
also use the program of Section~\ref{sec:ex1} to compute the conditional
entropy:

\begin{center}
    $H(X_{n+1} | X_n) = H(X_n, X_{n+1}) - H(X_n)$.
\end{center}

For \thesisTXT, we have:

\begin{center}
    $H(X_{n+1} | X_n) = 3.404$.
\end{center}


\section{Compression with known distributions}\label{sec:ex3}

We know \cite[p.~21]{it4} that the number of bits $b$ that an arithmetic coder
will need to encode any sequence $s$ with probability $p(s)$ follows:

\begin{center}
    $b < -\log p(s) + 2$
\end{center}

If we assume that all characters in a file $f$ are generated independent and
identically distributed (\iid) from $p(x_n)$ we can compute the probability of
the entire file as:

\begin{center}
    $p(f) = \prod\limits_{x_n \in f} p(x_n)$
    $\log p(f) = \sum\limits_{x_n \in f} \log p(x_n)$
\end{center}

We can now compute the maximum number of bits $b_1$ that we will need to encode
$f$ with an arithmetic coder using this unigram model:

\begin{center}
    $b_1 < -\log p(f) + 2 = 2 - \sum\limits_{x_n \in f} \log p(x_n)$
\end{center}

Performing this computation for \thesisTXT gives us:

\begin{center}
    $b_1 < 993,859$ bits
\end{center}

If we instead assume that only $x_0$, the first character in $f$, is drawn all
from $p(x_n)$ and all subsequent characters $x_1, x_2, \cdots, x_N$ are drawn
from $p(x_{n+1} | x_n)$, we can write the probability of the file as:

\begin{center}
    $p(f) = p(x_0) \cdot \prod\limits_{n = 1}^{N} p(x_{n + 1} | p(x_n))$
    $\log p(f) = \log p(x_0) + \sum\limits_{n = 1}^{N} \log p(x_{n + 1} | p(x_n))$
\end{center}

Using Bayes Rule we can re-write this as:

\begin{center}
    $\log p(f)
    = \log p(x_0) + \sum\limits_{n = 1}^{N} \log {p(x_n, x_{n + 1}) \over p(x_n)}
    = \log p(x_0) + \sum\limits_{n = 1}^{N} \log p(x_n, x_{n + 1}) - \log p(x_n)$
\end{center}

Like previously, we can now compute the maximum number of bits $b_2$ that an
arithmetic coder will need to encode $f$ using this bigram model:

\begin{center}
    $b_2 < -\log p(f) + 2
    = 2 - \log p(x_0) - \sum\limits_{n = 1}^{N} \log p(x_n, x_{n + 1}) + \log p(x_n)$
\end{center}

Performing this computation for \thesisTXT gives us:

\begin{center}
    $b_2 < 811,810$ bits
\end{center}

We use 13\% fewer bits using the bigram model instead of the unigram model!


\section{Compression with limited precision header}\label{sec:ex4}

Let us now consider the following coding scheme presented in \cite{it-assign}:
as previously, we will encode the file $f$ using a probability distribution
$p(i)$ over character-unigrams in the file, but we now also include a description
of this distribution as a header in the file. In order to simplify things, we
round the probabilities $p(i)$ to the next largest power of two $q^*(i)$ that we
can represent with 8 bits. Normalizing $q^*(i)$ to make the probabilities sum to
1 gives us the distribution $q(i)$ used for encoding:

\begin{center}
    $q^*(i) = {1 \over 2^8} \cdot \ceil{2^8 \cdot p(i)}$
    $q(i) = {1 \over \sum\limits_{j} q^*(j)} \cdot q^*(i)$
\end{center}

As previously, the probability of the entire file $f$ is:

\begin{center}
    $p(f) = \prod\limits_{x_n \in f} q(x_n)$
    $\log p(f) = \sum\limits_{x_n \in f} \log q(x_n)$
\end{center}

Using a similar approach to Section~\ref{sec:ex3} we can thus compute the
maximum number of bits $b_q$ necessary to encode $f$ using an arithmetic coder:

\begin{center}
    $b_q < -\log p(f) + 2
    = 2 - \sum\limits_{x_n \in f} \log q(x_n)$
\end{center}

If we assume that the alphabet $\mathcal{A} = \lbr a, b, \cdots, z, `` '' \rbr$
used in the file is common knowledge, we can represent the header in exactly
$(\norm{\mathcal{A}} - 1) \times 8 = 26 \times 8 = 208$ bits\footnotemark.
\footnotetext{We have 27 elements in $\mathcal{A}$ but since since probabilities
sum to one we can omit the last element of the header (it is simply one minus
the sum of the other elements), saving 8 bits.}

Applying all of the above to \thesisTXT we get $b_q < 994,722$ bits. We thus
need $b_q + 208 < 994,930$ bits in total to encode the file.


\section{Compression with adaptation}\label{sec:ex5}


\part{Noisy channel coding}

\section{\XOR-ing packets}\label{sec:ex6}

Performing the \XOR operation between the string ``nutritious snacks'' and the
bytes with numerical values ``59 6 17 0 83 84 26 90 64 70 25 66 86 82 90 95 75''
results in the string ``User: s5559183948''

\section{Decoding packets from a digital fountain}\label{sec:ex7}

A ``digital fountain'' is a code where parts of the source message are \XOR-ed
together before being sent. Appendix~\ref{app:ex7} proposes a Python program to
decode this code. On a high level, the program works as follows:

\begin{enumerate}
\item We are given as inputs a list of received packets and a list of source
      packets that were used to encode each of the received packets.
\item Find a packet that was encoded using only one source packet $s$, say,
      packet $n$.  If no such packet exists, the message can't be decoded.
\item We now know that the $n$\textsuperscript{th} character of the original
      message has to be the character with byte representation $v$.
\item Find all the packets that used packet $n$ as part of their encoding.
      \XOR these packets with $v$ and delete $n$ from their encoding list.
\item Repeat steps 2-4 until all the encoding lists are empty or until we find
      that the message can't be decoded.
\end{enumerate}

Using the program to decode the received bytes in \receivedTXT under the
assumption of the transformations in \packetsTXT produces the string ``Password:
X!3baA1z''. During the execution of the program, packets 2, 3, 4, 5, 6, 7, 8, 9,
10, 11, 12, 13, 14, 15, 16, 17, 18 and 19 are used (leaving packets 1, 20, 21,
22 and 23 unused).

\section{Creating a code}\label{sec:ex8}


\begin{thebibliography}{9}

\bibitem{mackay}
    D.J.C. MacKay,
    \emph{Information Theory, Inference \& Learning Algorithms},
    Cambridge University Press,
    2002.

\bibitem{it4}
    I. Murray,
    \emph{Practical compression 2: stream codes},
    Lecture at the University of Edinburgh,
    15-22 Oct 2013,
    available on-line at \url{http://goo.gl/Q2JkXm}

\bibitem{it-assign}
    I. Murray,
    \emph{Information Theory â€” Assessed Assignment},
    University of Edinburgh,
    21 Oct 2013,
    available on-line at \url{http://goo.gl/7QjRMc}

\bibitem{oed}
    Oxford Dictionary,
    \emph{What is the frequency of the letters in the alphabet in English?},
    Oxford University Press,
    retrieved 22 Oct 2013 from \url{http://goo.gl/DIl3uQ}

\end{thebibliography}


\onecolumn
\appendixpage
\appendix

\section{Code listing}

\subsection{ex1.py}\label{app:ex1}
\inputminted{python}{../src/ex1.py}
\newpage

\subsection{ex2.py}\label{app:ex2}
\inputminted{python}{../src/ex2.py}
\newpage

\subsection{ex3.py}\label{app:ex3}
\inputminted{python}{../src/ex3.py}
\newpage

\subsection{ex4.py}\label{app:ex4}
\inputminted{python}{../src/ex4.py}
\newpage

\subsection{ex6.py}\label{app:ex6}
\inputminted{python}{../src/ex6.py}
\newpage

\subsection{ex7.py}\label{app:ex7}
\inputminted{python}{../src/ex7.py}
\newpage

\subsection{util.py}\label{app:util}
\inputminted{python}{../src/util.py}

\end{document}

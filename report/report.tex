\documentclass[10pt,a4paper,twoside,twocolumn]{article}

\usepackage{appendix}
\usepackage{minted}
\usepackage{titlesec}
\usepackage{xspace}
\usepackage[margin=0.9in]{geometry}

\renewcommand*{\thepart}{\arabic{part}}
\titleformat{\part}[hang]
{\normalfont\Large\bf}{\partname\ \thepart:}{0.5em}{}[]
\titleformat{\section}[hang]
{\normalfont\normalsize\bf}{\thesection.}{0.5em}{}[]

\setlength{\parindent}{0pt}

\newcommand*{\thesisTXT}{{\tt thesis.txt}\xspace}
\newcommand*{\eg}{e.g.\xspace}

\title{Information Theory - Assessment 1}
\author{Clemens Wolff (s0942284)}
\date{\vspace{-2em}}

\begin{document}
\maketitle


\part{Source coding}

\section{Character statistics}\label{sec:ex1}
The Python program proposed in Appendix~\ref{app:ex1} computes a probability
distribution over characters from a given input file by counting the number of
occurrences $c_{x_n}$ of each character $x_n$ in the file and normalizing those
counts by the total number of characters to obtain:

\begin{center}
    $p(x_n) = {c_{x_n} \over\sum\limits_{x_n} c_{x_n}}$
\end{center}

\begin{table}[ht]
\centering
\begin{tabular}{| c c | c c |}
\hline
$x_n$ & $p(x_n)$ & $x_n$ & $p(x_n)$ \\
\hline
 ' '  & 0.166558 &   n   & 0.056932 \\
  a   & 0.068588 &   o   & 0.059740 \\
  b   & 0.014909 &   p   & 0.026120 \\
  c   & 0.025466 &   q   & 0.002442 \\
  d   & 0.027327 &   r   & 0.053147 \\
  e   & 0.094014 &   s   & 0.058728 \\
  f   & 0.016226 &   t   & 0.076727 \\
  g   & 0.017804 &   u   & 0.020545 \\
  h   & 0.030597 &   v   & 0.009116 \\
  i   & 0.071036 &   w   & 0.010950 \\
  j   & 0.001808 &   x   & 0.008732 \\
  k   & 0.005770 &   y   & 0.010464 \\
  l   & 0.036346 &   z   & 0.002247 \\
  m   & 0.027664 &       &          \\
\hline
\end{tabular}
\caption{Unigram probabilities for \thesisTXT}
\label{tbl:unigram-probs}
\end{table}

Using this distribution, the program then computes the entropy:

\begin{center}
    $H(X_n) = -\sum\limits_{x_n} p(x_n) \cdot log~p(x_n)$
\end{center}

For \thesisTXT, we have:

\begin{center}
    $H(X_n) = 4.168$
\end{center}

\section{Bigram statistics}\label{sec:ex2}
If we split a text into its constituent bigrams as a pre-procesing step, we can
use the program of Section~\ref{sec:ex1} to compute the joint entropy $H(X_n,
X_{n+1})$ of character bigrams in a given file. For \thesisTXT, we find:

\begin{center}
    $H(X_n, X_{n+1}) = 7.572 \approx 1.82H(X_n)$
\end{center}

As expected, this number is lower than $2H(X_n)$. By $p(x,y) \le p(x) \cdot
p(y)$ and monotonicity of the logarithm we have $\forall X,Y: H(X, Y) \le H(X) +
H(Y)$ with equality only occuring if $X$ and $Y$ are independent
\cite[p.~138]{mackay}. Here, we $X$ and $Y$ are occurrences of characters in
English text which are not independent \cite[p.~22-24]{mackay}.

Rearanging the chain rule of entropy \cite[p.~139]{mackay} reveals that we can
also use the program of Section~\ref{sec:ex1} to compute the conditional
entropy:

\begin{center}
    $H(X_{n+1} | X_n) = H(X_n, X_{n+1}) - H(X_n)$.
\end{center}

For \thesisTXT, we have:

\begin{center}
    $H(X_{n+1} | X_n) = 3.404$.
\end{center}


\section{Compression with known distributions}\label{sec:ex3}
\section{Compression with limited precision header}\label{sec:ex4}
\section{Compression with adaptation}\label{sec:ex5}


\part{Noisy channel coding}

\section{XOR-ing packets}\label{sec:ex6}
\section{Decoding packets from a digital fountain}\label{sec:ex7}
\section{Creating a code}\label{sec:ex8}


\begin{thebibliography}{9}

\bibitem{mackay}
    D.J.C. MacKay,
    \emph{Information Theory, Inference \& Learning Algorithms},
    Cambridge University Press,
    2002.

\end{thebibliography}


\onecolumn
\appendixpage
\appendix

\section{Code listing}

\subsection{ex1.py}\label{app:ex1}
\inputminted{python}{../src/ex1.py}

\subsection{ex2.py}\label{app:ex2}
\inputminted{python}{../src/ex2.py}

\subsection{ex6.py}\label{app:ex6}
\inputminted{python}{../src/ex6.py}

\subsection{ex7.py}\label{app:ex7}
\inputminted{python}{../src/ex7.py}

\subsection{util.py}\label{app:util}
\inputminted{python}{../src/util.py}

\end{document}

\documentclass[10pt,a4paper,oneside,onecolumn]{article}

\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{appendix}
\usepackage{minted}
\usepackage{titlesec}
\usepackage{xspace}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{lastpage}
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhead{}
\renewcommand{\headrulewidth}{0pt}
\cfoot{\thepage/\pageref{LastPage}}
\renewcommand*{\thepart}{\arabic{part}}
\titleformat{\part}[hang]
{\normalfont\Large\bf}{\partname\ \thepart:}{0.5em}{}[]
\titleformat{\section}[hang]
{\normalfont\normalsize\bf}{\thesection.}{0.5em}{}[]
\setlength{\parindent}{0pt}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newcommand*{\thesisTXT}{{\tt thesis.txt}\xspace}
\newcommand*{\receivedTXT}{{\tt received.txt}\xspace}
\newcommand*{\packetsTXT}{{\tt packets.txt}\xspace}
\newcommand*{\XOR}{{\tt XOR}\xspace}
\newcommand*{\eg}{e.g.\@}
\newcommand*{\ie}{i.e.\@}
\newcommand*{\iid}{i.i.d.\@}
\newcommand*{\norm}[1]{\ensuremath{\left|\left|#1\right|\right|}}
\newcommand*{\lbr}{\ensuremath{\left\{}}
\newcommand*{\rbr}{\ensuremath{\right\}}}
\newcommand*{\textapprox}{\raisebox{0.5ex}{\texttildelow}}

\title{Information Theory -- Assessment 1}
\author{Clemens Wolff (s0942284)}
\date{\vspace{-2em}}

\begin{document}
\maketitle\thispagestyle{fancy}


\part{Source coding}


\section{Character statistics}\label{sec:ex1}

The Python program proposed in Appendix~\ref{app:ex1} computes a probability
distribution $p$ over characters $x_n$ from a given input file by counting the
number of occurrences $c_{x_n}$ of each character $x_n$ in the file and
normalising those counts by the total number of characters:
\begin{equation}
    p(x_n) = {c_{x_n} \over\sum\limits_{x_n} c_{x_n}}.
\end{equation}
Using this distribution\footnotemark, the program then computes the entropy:
\footnotetext{Sanity check: ``e'', ``t'', ``a'' and ``o'' are indeed the most
common letters in English \cite{oed}.}
\begin{equation}
    H(X_n) = -\sum\limits_{x_n} p(x_n) \cdot \log p(x_n).
\end{equation}
For \thesisTXT, we have $H(X_n) = 4.17$ bits.\footnotemark
\footnotetext{Sanity check: this is pretty close to the ``4.11'' in
\cite[p.~7]{it4}.}

\begin{table}[ht]
\centering
\begin{tabular}{| c c | c c |}
\hline
$x_n$ & $p(x_n)$ & $x_n$ & $p(x_n)$ \\
\hline
 ' '  & 0.166558 &   n   & 0.056932 \\
  a   & 0.068588 &   o   & 0.059740 \\
  b   & 0.014909 &   p   & 0.026120 \\
  c   & 0.025466 &   q   & 0.002442 \\
  d   & 0.027327 &   r   & 0.053147 \\
  e   & 0.094014 &   s   & 0.058728 \\
  f   & 0.016226 &   t   & 0.076727 \\
  g   & 0.017804 &   u   & 0.020545 \\
  h   & 0.030597 &   v   & 0.009116 \\
  i   & 0.071036 &   w   & 0.010950 \\
  j   & 0.001808 &   x   & 0.008732 \\
  k   & 0.005770 &   y   & 0.010464 \\
  l   & 0.036346 &   z   & 0.002247 \\
  m   & 0.027664 &       &          \\
\hline
\end{tabular}
\caption{Unigram probabilities for \thesisTXT}
\label{tbl:unigram-probs}
\end{table}


\section{Bigram statistics}\label{sec:ex2}

If we split a text into its constituent bigrams as a pre-processing step, we can
use the program of Section~\ref{sec:ex1} to compute the joint entropy $H(X_n,
X_{n+1})$ of character bigrams in a given file. For \thesisTXT, we
find: $H(X_n, X_{n+1}) = 7.57 \approx 1.82H(X_n)$ bits.\footnotemark
\footnotetext{Sanity check: this is pretty close to the ``7.6'' in
\cite[p.~10]{it4}.}

As expected, this number is lower than $2H(X_n)$. By $p(x,y) \le p(x) \cdot
p(y)$ and monotonicity of the logarithm we have $\forall X,Y: H(X, Y) \le H(X) +
H(Y)$ with equality only occurring if $X$ and $Y$ are independent
\cite[p.~138]{mackay}. Here, we $X$ and $Y$ are occurrences of characters in
English text which are not independent \cite[p.~22-24]{mackay}.

Rearranging the chain rule of entropy \cite[p.~139]{mackay} reveals that we can
also use the program of Section~\ref{sec:ex1} to compute the conditional
entropy:
\begin{equation}
    H(X_{n+1} | X_n) = H(X_n, X_{n+1}) - H(X_n).
\end{equation}
For \thesisTXT, we have: $H(X_{n+1} | X_n) = 3.40$ bits.


\section{Compression with known distributions}\label{sec:ex3}

We know \cite[p.~21]{it4} the number of bits $b$ that an arithmetic coder
will need to encode any sequence $s$ that has probability $p(s)$:
\begin{equation}
    b < -\log p(s) + 2.
\end{equation}

If we assume that all characters $x_0, x_1, \cdots, x_N$ in a file $f$ are
generated independent and identically distributed (\iid) from $p(x_n)$ we can
compute the probability of the entire file as:
\begin{equation}
    p(f) = \prod\limits_{x_n \in f} p(x_n).
\end{equation}
Taking the logarithm on both sides gives:
\begin{equation}\label{eq:unigram-logprob}
    \log p(f) = \sum\limits_{x_n \in f} \log p(x_n).
\end{equation}
We can now compute the maximum number of bits $b_1$ that we will need to encode
$f$ with an arithmetic coder using a unigram model:
\begin{equation}\label{eq:unigram-nbits}
    b_1 < -\log p(f) + 2 = 2 - \sum\limits_{x_n \in f} \log p(x_n).
\end{equation}
Performing this computation for \thesisTXT gives us: $b_1 < 1,433,834$
bits.\footnote{We round all $b$ up because we can't have fractional bits.}

If we instead assume that only $x_0$, the first character in $f$, is drawn all
from $p(x_n)$ and all subsequent characters $x_1, x_2, \cdots, x_N$ are drawn
from $p(x_{n+1} | x_n)$, we can write the probability of the file as:
\begin{equation}
    p(f) = p(x_0) \cdot \prod\limits_{n = 0}^{N - 1} p(x_{n + 1} | x_n).
\end{equation}
Taking the logarithm on both sides gives:
\begin{equation}
    \log p(f) =
    \log p(x_0) + \sum\limits_{n = 0}^{N - 1} \log p(x_{n + 1} | x_n).
\end{equation}
Using Bayes Rule we can rewrite this as:
\begin{equation}
    \log p(f)
    = \log p(x_0)
    + \sum\limits_{n = 0}^{N - 1} \log {p(x_n, x_{n + 1}) \over p(x_n)}.
\end{equation}
We can simplify this further by applying properties of the logarithm:
\begin{equation}\label{eq:bigram-logprob}
    \log p(f)
    = \log p(x_0)
    + \sum\limits_{n = 0}^{N - 1} \log p(x_n, x_{n + 1}) - \log p(x_n).
\end{equation}
We can now compute the maximum number of bits $b_2$ that an arithmetic coder
will need to encode $f$ using a bigram model:
\begin{equation}\label{eq:bigram-nbits}
    b_2 < -\log p(f) + 2
    = 2 - \log p(x_0)
    - \sum\limits_{n = 0}^{N - 1} \log p(x_n, x_{n + 1}) + \log p(x_n).
\end{equation}
Performing this computation for \thesisTXT gives us: $b_2 < 1,171,193$ bits. We
use 18\% fewer bits if we base our arithmetic coder on a bigram model instead of
a unigram model!


\section{Compression with limited precision header}\label{sec:ex4}

Let us now consider the ``limited precision header'' coding scheme presented in
\cite{it-assign}: as previously, we will encode the file $f$ using a probability
distribution $p$ over character unigrams $i$ in the file, but we now also
include a description of this distribution as a header in the file. In order to
simplify things, we round the probabilities $p(i)$ to the next largest power of
two ${\hat q}(i)$ that we can represent with 8 bits:
\begin{equation}
    {\hat q}(i) = {1 \over 2^8} \cdot \ceil{2^8 \cdot p(i)}.
\end{equation}
Normalising ${\hat q}(i)$ to make the probabilities sum to 1 gives us the
distribution $q(i)$ used for encoding:
\begin{equation}
    q(i) = {1 \over \sum\limits_{j} {\hat q}(j)} \cdot {\hat q}(i)
\end{equation}
We now have all the information necessary to apply equations
\eqref{eq:unigram-logprob} and \eqref{eq:unigram-nbits} in order to compute the
maximum number of bits $b_{q^1}$ necessary to encode $f$ using an arithmetic
coder.  For \thesisTXT we get $b_{q^1} < 1,435,079$ bits.

If we now assume that the alphabet $\mathcal{A} = \lbr a, b, \cdots, z,
\textvisiblespace \rbr$ used in the file is known by everyone (\ie all parties
agree that if they see a header containing probabilities $p_0, p_1, \cdots,
p_{\norm{\mathcal{A}}}$ they know which symbols to associate with the sequence
of probabilities), we can represent the header in exactly $\norm{\mathcal{A}}
\times 8 = 27 \times 8 = 216$ bits.\footnotemark

We thus need $b_{q^1} + 216 < 1,435,295$ bits in total to encode \thesisTXT.

\footnotetext{We have 27 elements in $\mathcal{A}$. Normally we could argue that
since probabilities sum to one we can omit the last element: it is simply one
minus the sum of the other elements. However, this argument is incorrect here
because we are sending un-normalised probabilities which will only sum to one if
${\hat q}(i) = q(i)$.}

Note that we can relax the assumption that the alphabet is known to everyone by
increasing the header length by $\norm{\mathcal{A}} \times N + 8$ bits. The
header then contains each element in the alphabet encoded in some way using $N$
bits per symbol (\eg using ASCII) followed by the probability of that element.
We also define some sequence of 8 bits as the ``end of header'' symbol. Using
{\tt 00000000} for this symbol would be a good choice since the knowledge that a
character in the encoded message has probability zero is useless. We would
therefore never use this number to communicate the probability of an actual
member of our probability distribution, making {\tt 00000000} an unambiguous
sign for the ``end of header'' symbol.

If we want to use a limited precision header to encode a distribution over
character bigrams, the header is much larger than in the unigram case: if we
have an alphabet of $\norm{\mathcal{A}}$ characters, we have
$\norm{\mathcal{A}}^2$ character bigrams (\ie we need to send
$\norm{\mathcal{A}}^2$ probabilities in the header -- a length of $5,832$
bits in the case of \thesisTXT). Note that the joint (bigram) distribution is
all we need to send as a header: we can infer the unigram probability
distribution (used to get the probability of the first symbol in the file) from
the joint distribution by marginalisation and given the bigram and unigram
distributions we can easily compute the conditional distributions required for
decoding subsequent bits of the message.

Applying equations \eqref{eq:bigram-logprob} and \eqref{eq:bigram-nbits} to
compute the maximum number of bits $b_{q^2}$ necessary to encode \thesisTXT
using $q$ shows that the penalty for using this incorrect distribution is also
higher in the bigram case: we need $b_{q^2} < 1,463,686$ bits to encode the
contents of \thesisTXT\@.

We thus need $b_{q^2} + 5,832 < 1,469,518$ bits in total to encode \thesisTXT.

\section{Compression with adaptation}\label{sec:ex5}

One way to avoid having to send a header describing the probability distribution
used to encode a file is to infer the distribution as we go along decoding. We
can do this, for example, using the Laplace prediction rule
\cite[p.~2]{it-assign}:
\begin{equation}
    p(x_{n+1}=a_i|x_{\le n}) = {k_i + 1 \over n + \norm{\mathcal{A}}},
\end{equation}
where $k_i$ is the number of times character $a_i$ occurs in the sub-string
$x_0, x_1, \cdots, x_n$.
The log probability of a file $f$ with characters $x_0, x_1, \cdots, x_N$ then
is:
\begin{equation}\label{eq:adaptive-unigram}
    \log p(f) =
    \sum\limits_{n = 0}^N \log {k_{x_n} + 1 \over n + \norm{\mathcal{A}}}.
\end{equation}
Appendix~\ref{app:ex5} implements a Python program that uses equations
\eqref{eq:adaptive-unigram} and \eqref{eq:unigram-nbits} to compute the maximum
number of bits $b_{a^1}$ required to encode a file with this scheme. For
\thesisTXT we find that $b_{a^1} < 1,434,024$. Note that we save roughly 1,000
bits over the unigram model in Section~\ref{sec:ex4} which is more than just the
size of the header.

Modifying this approach to make use of probabilities over character bigrams
instead of just unigrams is trivial: just replace equation
\eqref{eq:adaptive-unigram} with a bigram prediction model such as
\cite[p.~3]{it-assign}:
\begin{equation}\label{eq:adaptive-bigram}
    p(x_{n+1}=a_i|x_{n} = a_j, x_{<n})
    = {k_{i|j} + 1 \over n_j + \norm{\mathcal{A}}},
\end{equation}
where $k_{i|j}$ is the number of times we have seen the bigram $a_ja_i$ in
$x_0, x_1, \cdots, x_n$ and $n_j$ is the number of times we have conditioned a
probability on $a_j$ so far.
We can run the Python program of Appendix~\ref{app:ex5} using equations
\eqref{eq:adaptive-bigram} and \eqref{eq:unigram-nbits} to compute the maximum
number of bits $b_{a^2}$ required to encode a file with this scheme. For
\thesisTXT we find that $b_{a^2} < 1,175,380$. This is only \textapprox 4,000
bits (or less than 1\%) more than the number we found in Section~\ref{sec:ex3}
where the sender and receiver both knew the exact probability distribution used
to encode the message!


\part{Noisy channel coding}


\section{\XOR--ing packets}\label{sec:ex6}

Performing the \XOR operation between the string ``nutritious snacks'' and the
bytes with numerical values ``59 6 17 0 83 84 26 90 64 70 25 66 86 82 90 95 75''
results in the string ``User: s5559183948''


\section{Decoding packets from a digital fountain}\label{sec:ex7}

A ``digital fountain'' is a code where parts of the source message are \XOR--ed
together before being sent. Appendix~\ref{app:ex7} proposes a Python program
that acts as a decoder for this scheme. On a high level, the program works as
follows:

\begin{enumerate}
\item We are given as inputs a list of received packets and a list of source
      packets that were used to encode each of the received packets.
\item Find a packet that was encoded using only one source packet $s$, say,
      packet $n$.  If no such packet exists, the message can't be decoded.
\item We now know that the $n$\textsuperscript{th} character of the original
      message has to be the character with byte representation $v$.
\item Find all the packets that used packet $n$ as part of their encoding.
      \XOR these packets with $v$ and delete $n$ from their encoding list.
\item Repeat steps 2--4 until all the encoding lists are empty or until we find
      that the message can't be decoded.
\end{enumerate}

Using the program to decode \receivedTXT and \packetsTXT produces the string
``Password: X!3baA1z''. Packets 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
16, 17, 18 and 19 are used in the final message (leaving packets 1, 20, 21, 22
and 23 unused).


\section{Creating a code}\label{sec:ex8}

This section investigates whether it is possible to combine various simple error
correcting codes for the Binary Symmetric Channel (BSC) in order to achieve
higher performance than using only a single code.

We can defined a composite AB code based on codes A and B as follows:
\begin{itemize}
\item \emph{Encoding} Encode the message $M$ with A to get $M_A$, use $B$ to
encode $M_A$ to get $M_{AB}$ which is the transmitted bit-string.
\item \emph{Decoding} Decode the received message $R$ with B to get $R_B$ and
decode that in turn with A to get the decoded message $R_BA$.
\end{itemize}
We will consider the 7,4-Hamming coding (H), 3-repetition coding (R3) and
3-depth code ``interleaving'' (I) for our base codes.

7,4-Hamming coding sends message bits $m_1, m_2, m3_, m_4$ over the BSC,
protected by three parity bits $p_1 = m_1 \oplus m_2 \oplus m_4, p_2 = m_1
\oplus m_3 \oplus m_4, p3 = m_2 \oplus m_3 \oplus m_4$. The receiver of the can
correct up to one error in the message via syndrome decoding. The code has rate
${4 \over 7}$.

R-repetition coding sends message bit $m_1$ protected by R parity bits $p_i =
m_1$. The receiver can correct up to $\left\lceil{R \over 2}\right\rceil - 1$
errors in the message via majority vote decoding: decode any sequence of R
received bits as whichever element in the sequence is most common. The code has
rate ${1 \over R}$.

Combining a 7,4-Hamming code with a R-repetition code sends each of the
Hamming-code bits R times or encodes each of the R repetition-code bits with 7
Hamming-code bits.  The rate of this combined code is therefore ${4 \over 7
\times R}$.

D-depth interleaving arranges message bits $m_1, m_2, \cdots, m_n$ into an array
with D rows and ${n \over D}$ columns, filling each column with message bits
before starting a new row. The elements in the array are then sent column-wise
rather than row-wise over the channel.  Decoding repeats this process and
therewith undoes it. The code does not correct any errors itself, but using the
code to encode a message already encoded with some other underlying error
correction code U helps in dealing with multiple errors occurring in a row
(``burst errors''): interleaving spreads message and parity bits more apart
which decreases the likelihood that too many errors accumulate in a certain span
and therewith overwhelm the error correction capabilities of U. Using an
interleaving code with an underlying code U does not add any additional bits to
the message, the rate of the interleaving code is therefore the rate of U.

In order to evaluate the performance of the codes, we encode and send 500 random
messages over three BSCs with error probabilities $f = 0.001, 0.01, 0.1, 0.4$
respectively. The estimate of the bit-error rate of any of our coding schemes is
the average Hamming distance between the sent and received messages over the
average message length. Tables~\ref{tbl:code-errors} and~\ref{tbl:code-rates}
summarise the findings: we do not manage to outperform Hamming codes or
repetition codes in any interesting way shape or form. Combining Hamming and
repetition codes leads to a lower bit-error probability if compared to using a
Hamming code on its own, but the resulting code still has a poorer bit-error
probability and rate than a simple repetition code. The interleaving code does
not affect performance. This can probably be explained by the memory-less nature
of the BSC: unlike many real-world channels, the BSC adds noise at random to
messages sent through it, not in bursts; we thus do not expect interleaving to
improve performance on average.

\begin{table}[ht]
\centering
\begin{tabular}{| c | c | c |}
\hline
$f$ & Code & Bit-error probability \\
\hline
0.4 & H          & 0.42286325 \\
    & R3         & 0.3521785 \\
    & R3 + I     & 0.3520545 \\
    & H + I      & 0.4229125 \\
    & H + R3     & 0.3830005 \\
    & R3 + H     & 0.389885 \\
    & H + R3 + I & 0.3826605 \\
    & R3 + H + I & 0.389411 \\
\hline
0.1 & H          & 0.11281175 \\
    & R3         & 0.02810925 \\
    & R3 + I     & 0.0279555 \\
    & H + I      & 0.1129535 \\
    & H + R3     & 0.029242 \\
    & R3 + H     & 0.06481325 \\
    & H + R3 + I & 0.02926425 \\
    & R3 + H + I & 0.064571 \\
\hline
0.01 & H          & 0.010282 \\
     & R3         & 0.00028925 \\
     & R3 + I     & 0.00030425 \\
     & H + I      & 0.01027775 \\
     & H + R3     & 0.00031425 \\
     & R3 + H     & 0.0051465 \\
     & H + R3 + I & 0.00030175 \\
     & R3 + H + I & 0.005052 \\
\hline
0.001 & H          & 0.0009915 \\
      & R3         & 0.00000275 \\
      & R3 + I     & 0.0000035 \\
      & H + I      & 0.00101275 \\
      & H + R3     & 0.0000025 \\
      & R3 + H     & 0.00049925 \\
      & H + R3 + I & 0.0000025 \\
      & R3 + H + I & 0.000508 \\
\hline

\end{tabular}
\caption{Average code bit error probability}
\label{tbl:code-errors}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{| c | c |}
\hline
Code & Rate \\
\hline
H          & 0.571428571429 \\
R3         & 0.333333333333 \\
R3 + I     & 0.333333333333 \\
H + I      & 0.571428571429 \\
H + R3     & 0.190476190476 \\
R3 + H     & 0.190476190476 \\
H + R3 + I & 0.190476190476 \\
R3 + H + I & 0.190476190476 \\
\hline
\end{tabular}
\caption{Average code rate}
\label{tbl:code-rates}
\end{table}

Let us now analyse the code that performed best in our experimental analysis a
bit more rigorously: H + R3 (the 7,4-Hamming code where each bit gets repeated
three times). Let us reason about performance taking a sequence we want to
encode and send as an example. Say we want to send {\tt 0001}. The Hamming code
transforms this into {\tt 0001011}. The repetition code then transforms this
into {\tt 000 000 000 111 000 111 111} which is the message we will transmit.
Assume that we send the message over a Binary Symmetric Channel with bit-flip
probability f.

When will the channel not garble the message in a way that will lead to a
decoding error? In order to reason about this probability, let us focus on the 7
length 3 sub-blocks of the encoded message first.

Clearly, if the channel does not flip any bits, we will be able to recover the
original message. This happens with probability $(1-f)^3$ for any 3-sub-block.

Our outer (repetition) code will correct any single bit error in any
3-sub-block. We will experience this situation with probability ${3 \choose 1}
f(1-f)^2 = 3f(1-f)^2 = 1 - (1-f)^2(1 - 4f)$.

This means that any 3-sub-block has probability $p_3$ of being corrupted in a
way that will elude out outer code.
\begin{equation}
p_3 = 1 - (1-f)^3 - 3f(1-f)^2.
\end{equation}

Reminding ourselves that our message consists of seven 3-sub-blocks, we can
express the probability that exactly x of these blocks are corrupted as
$p_{n=x} = {7 \choose x} p_3^x(1-p_3)^{7-x}$.

If one or less 3-sub-blocks in the message are corrupted, our inner (Hamming)
code can recover this. The probability of this event occurring is $p_{n \leq 1} =
p_{n=0} + p_{n=1}$. This is also the probability that our channel will not
garble the message in an unrecoverable way. Whence we derive the message
recovery probability $p_r$.
\begin{equation}\label{eq:precover}
\begin{split}
p_r = p_{n \leq 1} =
{7 \choose 0} p_3^0(1-p_3)^{7-0} + {7 \choose 1} p_3^1(1-p_3)^{7-1} \\
= (1-p_3)^7 + 7p_3(1-p_3)^6 = (1-p_3)^6(1+6p_3) \\
= ((1-f)^3+3f(1-f)^2)^6(1+6(1 - (1-f)^3 - 3f(1-f)^2))
\end{split}
\end{equation}

Table~\ref{tbl:precover} evaluates Equation~\eqref{eq:precover} for various
values of $f$.

\begin{table}[ht]
\centering
\begin{tabular}{| c | c |}
\hline
$f$ & $p_r$ \\
\hline
0.4   & 0.230 \\
0.1   & 0.985 \\
0.01  & 0.999 \\
0.001 & 0.999 \\
\hline
\end{tabular}
\caption{Message recovery probability}
\label{tbl:precover}
\end{table}


\begin{thebibliography}{9}

\bibitem{mackay}
    D.J.C. MacKay,
    \emph{Information Theory, Inference \& Learning Algorithms},
    Cambridge University Press,
    2002.

\bibitem{it4}
    I. Murray,
    \emph{Practical compression 2: stream codes},
    Lecture at the University of Edinburgh,
    15-22 Oct 2013,
    available online at \url{http://goo.gl/Q2JkXm}

\bibitem{it-assign}
    I. Murray,
    \emph{Information Theory â€” Assessed Assignment},
    University of Edinburgh,
    21 Oct 2013,
    available online at \url{http://goo.gl/7QjRMc}

\bibitem{oed}
    Oxford Dictionary,
    \emph{What is the frequency of the letters in the alphabet in English?},
    Oxford University Press,
    retrieved 22 Oct 2013 from \url{http://goo.gl/DIl3uQ}

\end{thebibliography}


\onecolumn
\appendixpage
\appendix

\section{Code listing}

\subsection{ex1.py}\label{app:ex1}
\inputminted{python}{../src/ex1.py}

\subsection{ex2.py}\label{app:ex2}
\inputminted{python}{../src/ex2.py}

\subsection{ex3.py}\label{app:ex3}
\inputminted{python}{../src/ex3.py}

\subsection{ex4.py}\label{app:ex4}
\inputminted{python}{../src/ex4.py}

\subsection{ex5.py}\label{app:ex5}
\inputminted{python}{../src/ex5.py}

\subsection{ex6.py}\label{app:ex6}
\inputminted{python}{../src/ex6.py}

\subsection{ex7.py}\label{app:ex7}
\inputminted{python}{../src/ex7.py}

\subsection{ex8.py}\label{app:ex8}
\inputminted{python}{../src/ex8.py}

\subsection{util.py}\label{app:util}
\inputminted{python}{../src/util.py}

\end{document}

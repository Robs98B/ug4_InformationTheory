\documentclass[10pt,a4paper,twoside,twocolumn]{article}

\usepackage{appendix}
\usepackage{minted}
\usepackage{titlesec}
\usepackage{xspace}
\usepackage[margin=0.9in]{geometry}

\renewcommand*{\thepart}{\arabic{part}}
\titleformat{\part}[hang]
{\normalfont\Large\bf}{\partname\ \thepart:}{0.5em}{}[]
\titleformat{\section}[hang]
{\normalfont\normalsize\bf}{\thesection.}{0.5em}{}[]

\setlength{\parindent}{0pt}

\newcommand*{\thesisTXT}{{\tt thesis.txt}\xspace}
\newcommand*{\eg}{e.g.\xspace}

\title{Information Theory - Assessment 1}
\author{Clemens Wolff (s0942284)}
\date{\vspace{-2em}}

\begin{document}
\maketitle


\part{Source coding}

\section{Character statistics}\label{sec:ex1}
The Python program proposed in Appendix~\ref{app:ex1} computes a probability
distribution over characters from a given input file by counting the number of
occurrences $c_{x_n}$ of each character $x_n$ in the file and normalizing those
counts by the total number of characters $C = \sum\limits_{x_n} c_{x_n}$ to
obtain $p(x_n) = {1 \over C} \cdot c_{x_n}$.

It then computes the entropy $H(X_n)$ of the distribution using the formula:
\begin{center}
    $H(X_n) = -\sum\limits_{x_n} p(x_n) \cdot log~p(x_n)$
\end{center}
Analysing the file \thesisTXT with this program we find:
\begin{center}
    $H(X_n) = 4.168$
\end{center}

\section{Bigram statistics}\label{sec:ex2}
If we split a text into its constituent bigrams as a pre-procesing step we can
use the program of Section~\ref{sec:ex1} to compute the joint entropy $H(X_n,
X_{n+1})$ of character bigrams in a given file.

We expect this number to be lower than $2H(X_n)$ because by $p(x,y) \le
p(x) \cdot p(y)$ and monotonicity of the logarithm we have $\forall X,Y: H(X, Y)
\le H(X) + H(Y)$ with equality only occuring if $X$ and $Y$ are independent
\cite[p.~138]{mackay}. Here, we $X$ and $Y$ are occurrences of characters in
English text which are not independent \cite[p.~22-24]{mackay}.

Analyzing \thesisTXT, we find:
\begin{center}
    $H(X_n, X_{n+1}) = 7.572 \approx 1.82H(X_n)$
\end{center}

Rearanging the chain rule of entropy \cite[p.~139]{mackay} reveals that we can
also use the program of Section~\ref{sec:ex1} to compute the conditional
entropy:
\begin{center}
    $H(X_{n+1} | X_n) = H(X_n, X_{n+1}) - H(X_n)$.
\end{center}
For \thesisTXT, we have:
\begin{center}
    $H(X_{n+1} | X_n) = 3.404$.
\end{center}


\section{Compression with known distributions}\label{sec:ex3}
\section{Compression with limited precision header}\label{sec:ex4}
\section{Compression with adaptation}\label{sec:ex5}


\part{Noisy channel coding}

\section{XOR-ing packets}\label{sec:ex6}
\section{Decoding packets from a digital fountain}\label{sec:ex7}
\section{Creating a code}\label{sec:ex8}


\begin{thebibliography}{9}

\bibitem{mackay}
    D.J.C. MacKay,
    \emph{Information Theory, Inference \& Learning Algorithms},
    Cambridge University Press,
    2002.

\end{thebibliography}


\onecolumn
\appendixpage
\appendix

\section{Code listing}

\subsection{ex1.py}\label{app:ex1}
\inputminted{python}{../src/ex1.py}

\subsection{ex2.py}\label{app:ex2}
\inputminted{python}{../src/ex2.py}

\subsection{ex6.py}\label{app:ex6}
\inputminted{python}{../src/ex6.py}

\subsection{ex7.py}\label{app:ex7}
\inputminted{python}{../src/ex7.py}

\subsection{util.py}\label{app:util}
\inputminted{python}{../src/util.py}

\end{document}
